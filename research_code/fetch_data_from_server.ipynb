{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS: \n",
    "- connect to VPN and type in the terminal :  *ssh cleres@iccluster028.iccluster.epfl.ch*\n",
    "- see the list of datasets: hadoop fs -ls /datasets\n",
    "- Create your python script \n",
    "- **put the .py script on the cluster** : scp -r -p FILE YOURNAME@iccluster028.iccluster.epfl.ch:/home/YOURNAME/FILE\n",
    "- **Run the script on the cluster** : spark-submit --master yarn --deploy-mode cluster --driver-memory 4G --num-executors 5 --executor-memory 4G --executor-cores 5 fetch_data_from_server.py\n",
    "- **Download the output from the cluster** : scp -r -p  cleres@iccluster028.iccluster.epfl.ch:/home/cleres/posts.parquet ./\n",
    "- See the size of the files that we dowload : **du -h posts.parquet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWITTER dataset\n",
    "What do we need to do : \n",
    "- The data is on the cluster in the **datasets/** directory \n",
    "- Pandas is perfect for data analysis and manipulation, while Spark can do the heavy job. The ideal workflow is to write your aggregation task in Spark, save the reduced dataset in JSON format and then load it on your laptop with Pandas.\n",
    "- How can I **access local files in the cluster**, I try to access\n",
    "user/greffe/data/mapping_country_codes.csv\n",
    "with \n",
    "spark.read.csv(\"/home/greffe/data/mapping_country_codes.csv\", header=True)\n",
    "but get the following error:\n",
    "pyspark.sql.utils.AnalysisException: u'Path does not exist: hdfs://iccluster028.iccluster.epfl.ch:8020/user/greffe/data/mapping_country_codes.csv;'\n",
    "EDIT: I understood the problem\n",
    "1) we have to put the files on hdfs \"hadoop fs -put file\"\n",
    "2) we access those at \"hdfs:///user/USERNAME/FILE\" from spark (edited)\n",
    "\n",
    "- **put the py file on the cluster** (in a new terminal window without ssh access) : scp -r -p FILE YOURNAME@iccluster028.iccluster.epfl.ch:/home/YOURNAME/FILE\n",
    "- OUR VERSION : scp -r -p fetch_data_from_server.py cleres@iccluster028.iccluster.epfl.ch:/home/cleres/\n",
    "\n",
    "\n",
    "- spark-submit --master yarn --deploy-mode cluster --driver-memory 4G --num-executors 5 --executor-memory 4G --executor-cores 5 some_script.py script_arg1 script_arg2\n",
    "- OUR VERSION: **spark-submit --master yarn --deploy-mode cluster --driver-memory 4G --num-executors 5 --executor-memory 4G --executor-cores 5 fetch_data_from_server.py**\n",
    "\n",
    "- How could we **download the output from the cluster**? you have to transfering from the hdfs to your local file system with hadoop and then to use scp to get it locally (if you use files in the py script, they have to be on the hdfs by the way)Export the files from user/cleres/ to the local machine: **scp -r -p  cleres@iccluster028.iccluster.epfl.ch:/home/cleres/posts.parquet ./**\n",
    "\n",
    "- to do this you need to use copy-to-local before \n",
    "\n",
    "1.hadoop fs -get HDFS file path      Local system directory path\n",
    "\n",
    "2.hadoop fs -copyToLocal HDFS file path    Local system directory path\n",
    "\n",
    "\n",
    "NOTE: \"./\" exports the files in the directory where the UNIX command is launched. \n",
    "\n",
    "- see the size of the files that we dowload : **du -h posts.parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import unix_timestamp, udf, to_date\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'UTC')\n",
    "sc = spark.sparkContext\n",
    "\n",
    "CLUSTER_DATA_DIR = '/datasets/twitter_internetarchive/2017/01/01/01/'\n",
    "MY_DATA_DIR = '/data/'\n",
    "\n",
    "files = os. listdir(path)\n",
    "for name in files:\n",
    "    print(path + name)\n",
    "    posts_df = spark.read.json(CLUSTER_DATA_DIR + name)\n",
    "    posts_df.write.parquet(MY_DATA_DIR + name + \".parquet\", mode = \"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
