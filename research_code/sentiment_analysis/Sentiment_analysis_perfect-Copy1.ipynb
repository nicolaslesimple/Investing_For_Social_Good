{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "It could be interesting to correlate the greater public's interest into social good projects expressed on social media platforms such as Twitter and Facebook, or news platforms, with the actual investments that are done. \n",
    "To add the public opinion part, sentiment analysis was done on tweet from 2017. In fact, sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. In this part of the data story, the text classifier process tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral will be describe.\n",
    "\n",
    " To do that, several questions needed to be answered. \n",
    "    - How to make a ranking of “public opinion” regarding a company?\n",
    "    - What is the great public's opinion regarding the companies and investors ? \n",
    "    - Just capital ranking is one thing. This metrics is complex and tries to take into account a high number of variables. But it just capital score correlated with public opinion ? It means that we will try to understand the public’s opinion regarding the companies that held many investments in social good versus those who do not invest in social good?\n",
    " -SI JA I LE TEMPS CETTE NUIT IS A CORELATION BETWEEN THESES SCORES AND GEOGRAPHICAL INFORMATION EXISTING ? VOIR AVEC DAVE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from translate import translator\n",
    "from googletrans import Translator\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "import networkx as nx\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import fastText\n",
    "import string\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "import itertools\n",
    "from community import community_louvain\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacOSFile(object):\n",
    "    '''\n",
    "    This function allows the saving and the loading of huge files.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    '''\n",
    "    This function allows the saving of huge files to pickle.\n",
    "    '''\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    '''\n",
    "    This function allows the loading of huge files to pickle.\n",
    "    '''\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing to clean text data.\n",
    "    \n",
    "    In particular:\n",
    "    - lowers the string\n",
    "    - removes the character [']\n",
    "    - replaces punctuation characters with spaces\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\'\", \"\", text)  # remove the character [']\n",
    "\n",
    "    # removing the punctuation\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    split = \" \"\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        translate_map = dict((ord(c), str(split)) for c in filters)\n",
    "        text = text.translate(translate_map)\n",
    "    elif len(split) == 1:\n",
    "        translate_map = maketrans(filters, split * len(filters))\n",
    "        text = text.translate(translate_map)\n",
    "    else:\n",
    "        for c in filters:\n",
    "            text = text.replace(c, split)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(tweet):\n",
    "    \"\"\"\n",
    "    This function allows the translation of a string to english language\n",
    "    Arg :\n",
    "        -tweet : String corresponding to the string we want to translate.\n",
    "    \"\"\"\n",
    "    tw_translated = translator('en', tweet)\n",
    "    return tw_translated\n",
    "\n",
    "def translate(tweet,language):\n",
    "    '''\n",
    "    Other translate function using google API.\n",
    "    '''\n",
    "    translator = Translator()\n",
    "    tw_inggris = translator.translate(tweet, src=language, dest='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(filepath, extension):\n",
    "        '''\n",
    "        This function allows the loading of all the files in a folder corresponding to the Data we want to use.\n",
    "        Argument :\n",
    "            - filepath : String corresponding to the folder where we want to select all the files with type_of_file and extension string in their names.\n",
    "            - extension : String corresponding to the extension of the files we want to select and use. Should be '.p' or '.txt'.\n",
    "        Return :\n",
    "            - files : list of the files name order alphabetically that we will load.\n",
    "        '''\n",
    "        # Get all the files with the correponding type of file name:\n",
    "        all_files = os.listdir(filepath)\n",
    "        files = list()\n",
    "        for file in all_files:\n",
    "            if (extension in file):\n",
    "                files.append(file)\n",
    "        # Put them in alphabetical order:\n",
    "        files = sorted(files)\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_type(filepath, extension, type_of_file):\n",
    "        '''\n",
    "        This function allows the loading of all the files in a folder corresponding to the Data we want to use.\n",
    "        Argument :\n",
    "            - filepath : String corresponding to the folder where we want to select all the files with type_of_file and extension string in their names.\n",
    "            - extension : String corresponding to the extension of the files we want to select and use. Should be '.p' or '.txt'.\n",
    "            - type_of_file : String corresponding to the type of data we want to load. Should be 'Booking' or 'BOL'\n",
    "        Return :\n",
    "            - files : list of the files name order alphabetically that we will load.\n",
    "        '''\n",
    "        # Get all the files with the correponding type of file name:\n",
    "        all_files = os.listdir(filepath)\n",
    "        files = list()\n",
    "        for file in all_files:\n",
    "            if (extension in file)&(type_of_file in file):\n",
    "                files.append(file)\n",
    "        # Put them in alphabetical order:\n",
    "        files = sorted(files)\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english_tweet(df):\n",
    "    '''\n",
    "    This function allows the filtering of the tweet which are not written in english.\n",
    "    Arg :\n",
    "        - df : Raw dataframe from the tweet dataset we want to filter.\n",
    "    Return :\n",
    "        - df : Input dataframe filtered.\n",
    "    '''\n",
    "    df = df[df.lang=='en']\n",
    "    df = df.reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_json(archive_path, outfile_path):\n",
    "    outfile_path = outfile_path[:-4]\n",
    "    with open(archive_path, 'rb') as source, open(outfile_path, 'wb') as dest:\n",
    "        dest.write(bz2.decompress(source.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Load json data export from the clusters :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKKKK 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Desktop/06/21/00/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1bcf910b39f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../../Desktop/0{i}/{j}/0{k}/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bz2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7f7343cb0eea>\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(filepath, extension)\u001b[0m\n\u001b[1;32m     10\u001b[0m         '''\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Get all the files with the correponding type of file name:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Desktop/06/21/00/'"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "for i in np.arange(12):\n",
    "    for j in ['01','11','21']:\n",
    "        list_df = []\n",
    "        for k in range(24):\n",
    "            files = []\n",
    "            if k<10:\n",
    "                files = load_files(f'../../Desktop/0{i}/{j}/0{k}/', '.json')\n",
    "                for file in files:\n",
    "                    if file.find('bz2')==-1:\n",
    "                        list_df.append(pd.read_json(f'../../Desktop/0{i}/{j}/0{k}/{file}'))\n",
    "            else:\n",
    "                files = load_files(f'../../Desktop/0{i}/{j}/{k}/', '.json')\n",
    "                for file in files:\n",
    "                    if file.find('bz2')==-1:\n",
    "                        list_df.append(pd.read_json(f'../../Desktop/0{i}/{j}/{k}/{file}'))\n",
    "\n",
    "        if len(list_df)!=0:\n",
    "            df = pd.concat(list_df)\n",
    "            pickle_dump(df,f'../../data_twitter/load_tweet_2017_{i}_{j}.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "First of all, output of the last part where taken and mix with a new dataset which is the Twitter dataset given in the course. It contains 1% of the tweets of 2017 and it’s store on the ADA cluster. The documentation of Twitter API was analyzed to understand how to use this database.The group decided to work on a selected part of the entire dataset. In fact, our goal is to score the public’s opinion of compagnies thanks to a sentiment analysis done over tweets of 2017. To do that we decide to take 3 days per month written in english. English tweets were filter as our analysis is made on Americans compagnies. We thus care more about the public opinion of English speaking population as they are more impacted by theses compagnies. For each month, we take always the same days : the first one, the 11th one and the 21th. Theses two filter process allows us to work on a twitter dataset with the following attributes : \n",
    "ADD THE DESCRIPTION OF THE DATASET : Nan et … FAIRE PANDAS .DESCRIBE\n",
    "\n",
    "\n",
    "\n",
    "Then, to make it easy to use, we create a dictionary containing all the tweets and the tags cleaned. ‘Cleaned’ means that for each string, we lowers the string, we removes the character [‘], and we replaces punctuation characters with spaces.\n",
    "Then, the creation of a dictionary of all company and investor was be done and finally, the company or investor cleaned name was used as key of the dictionary, grouping all the tweets about it together.Tweets were filter using a easy rule : a tweet concerned a company if the name of the company is present in the tweet text or the the tweet tags. All this process gives us a dates with the following specificity : WRITE ALLL SPECIFICITIES\n",
    "PEUT ETRE \n",
    "\n",
    "\n",
    "\n",
    "## Create a dictionary containing each tweet with the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_tweet_2017_1_01.p\n"
     ]
    }
   ],
   "source": [
    "def dict_tweet_function(df):\n",
    "    list_hashtags = []\n",
    "    dict_tot = dict()\n",
    "    for i in range(df.text.index.max()):\n",
    "        print(f'{i} over {df.text.index.max()}')\n",
    "        dict_tmp = dict()\n",
    "        if (type(df.text[i])==str):\n",
    "            if (len(df.entities[i]['hashtags'])!=0):\n",
    "                dict_tmp['hashtags'] = df.entities[i]['hashtags'][0]['text']\n",
    "                dict_tmp['text'] = df.text[i]\n",
    "            else :\n",
    "                dict_tmp['hashtags'] = df.entities[i]['hashtags']\n",
    "                dict_tmp['text'] = df.text[i]\n",
    "            dict_tot[(i,-1)]=dict_tmp\n",
    "        elif type(df.text[i])==pd.core.series.Series: \n",
    "            for j in range(len(df.text[i])): \n",
    "                dict_tmp = dict()\n",
    "                if (type(df.text[i].reset_index().text[j])==str):\n",
    "                    if len(df.entities[i].reset_index().entities[j]['hashtags'])!=0:\n",
    "                        dict_tmp['hashtags'] = df.entities[i].reset_index().entities[j]['hashtags'][0]['text']\n",
    "                        dict_tmp['text'] = df.text[i].reset_index()['text'][j]\n",
    "                    else : \n",
    "                        dict_tmp['hashtags'] = df.entities[i].reset_index().entities[j]['hashtags']\n",
    "                        dict_tmp['text'] = df.text[i].reset_index()['text'][j]\n",
    "                    dict_tot[(i,j)]=dict_tmp\n",
    "        pickle_dump(dict_tot, f'../../data_twitter/dict/dict_all_tweet_2017_1_01.p')\n",
    "    return dict_tot\n",
    "\n",
    "df = pickle_load(f'../../data_twitter/load_tweet_2017_1_01.p')\n",
    "tmp = dict_tweet_function(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tot = pickle.load(open('dictionary_with_all_tweets_and_hashtags.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge several process files done in a segmented way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files(f'../../data_twitter/dict/', '.json')\n",
    "df = Dataframe()\n",
    "for file in files:\n",
    "    df = pd.concat(df, pickle_load(f'../../data_twitter/dict/{file}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the list of investor and compagny we are working on\n",
    "We just take compagnies/investor present in 2017 as our sentiment anlysis is done on this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning:\n",
      "\n",
      "Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing total_bytes=75136...\n",
      "writing bytes [0, 75136)... done.\n",
      "writing total_bytes=3768...\n",
      "writing bytes [0, 3768)... done.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../Desktop/13f_results_13-18Q3_merged.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['Year'] = df['date'].dt.year\n",
    "df = df[df.Year == 2017]\n",
    "df = df.reset_index()\n",
    "pickle_dump(df.investor.unique(), 'investor.p')\n",
    "pickle_dump(df.name.unique(), 'compagny.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we clean the name of the compagny/investor. In particular:\n",
    "    - lowers the string\n",
    "    - removes the special character \n",
    "    - replaces punctuation characters with spaces\n",
    "Then we split the string and select the longest word and take as name of the compagny the longest word one plus one word before. In fact, after analysis the dataset, this methods gives the best results compare to other trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('../../Desktop/13f_results_13-18Q3_merged.csv')\n",
    "compagny = tmp['name'].unique()\n",
    "investor = tmp['investor'].unique()\n",
    "\n",
    "# Clean the name : \n",
    "for i in range(len(compagny)):\n",
    "    index = clean_text(compagny[i]).split().index(max(clean_text(compagny[i]).split(), key=len))\n",
    "    if index > 0:\n",
    "        compagny[i] = f'{clean_text(compagny[i]).split()[index_name-1]} {clean_text(compagny[i]).split()[index_name]}'\n",
    "    else :\n",
    "        compagny[i] = clean_text(compagny[i]).split()[0]\n",
    "        \n",
    "for i in range(len(investor)):\n",
    "    index = clean_text(investor[i]).split().index(max(clean_text(investor[i]).split(), key=len))\n",
    "    if index > 1:\n",
    "        investor[i] = f'{clean_text(investor[i]).split()[index_name-2]} {clean_text(investor[i]).split()[index_name-1]} {clean_text(investor[i]).split()[index_name]}'\n",
    "    elif index > 0:\n",
    "        investor[i] = f'{clean_text(investor[i]).split()[index_name-1]} {clean_text(investor[i]).split()[index_name]}'\n",
    "    else :\n",
    "        investor[i] = clean_text(investor[i]).split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "5146\n"
     ]
    }
   ],
   "source": [
    "print(len(compagny))\n",
    "print(len(investor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary using the compagny or investor as key grouping all the tweets about it\n",
    "We consider that the tweet concerned a compagny/investor is there is his name in the tweet or the hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_per_category(list_of_names, dict_tot):\n",
    "    dict_per_category = dict()\n",
    "    i = 0\n",
    "    for name in list_of_names :\n",
    "        print(i)\n",
    "        list_tweet = []\n",
    "        for key in list(dict_tot.keys()) :\n",
    "            if (str(dict_tot[key]['hashtags']).lower().find(name.lower()) != -1) | (dict_tot[key]['text'].lower().find(name.lower()) != -1):\n",
    "                list_tweet.append(dict_tot[key]['text'])\n",
    "        dict_per_category[name] = list_tweet\n",
    "        i=i+1\n",
    "    return dict_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "writing total_bytes=43064800...\n",
      "writing bytes [0, 43064800)... done.\n"
     ]
    }
   ],
   "source": [
    "dict_tot = pickle_load(f'../../data_twitter/dict/dict_all_tweet_2017_1_01.p')\n",
    "dict_per_compagny = tweet_per_category(compagny, dict_tot)\n",
    "pickle_dump(dict_per_compagny, 'dictionary_per_compagny_tweet.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n"
     ]
    }
   ],
   "source": [
    "dict_tot = pickle_load(f'../../data_twitter/dict/dict_all_tweet_2017_1_01.p')\n",
    "dict_per_investor = tweet_per_category(investor, dict_tot)\n",
    "pickle_dump(dict_per_investor, 'dictionary_per_investor_tweet.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_per_compagny = pickle.load(open('dictionary_per_compagny_tweet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_per_investor = pickle.load(open('dictionary_per_investor_tweet.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of sentiment analysis :\n",
    "\n",
    "After this pre-processing part, sentiment analysis can begin. We used 2 different method. \n",
    "We apply theses two different method for each tweet and thus as output we have two different scores per tweet (Vader and Fastest). We do the mean of theses two scores. Then if there are several tweets, we take the mean. In fact, we want to penalized when we have extreme score.\n",
    "\n",
    "\n",
    "## Make sentiment analysis for each tweet for each compagny/investors :\n",
    "### Vader\n",
    "\n",
    "The first one is Vader. VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words. In this approach, each of the words in the lexicon is rated as to whether it is positive or negative, and in many cases, how positive or negative. As you might have guessed, when VADER analyses a piece of text it checks to see if any of the words in the text are present in the lexicon. VADER produces four sentiment metrics from these word ratings, which you can see below. The first three, positive, neutral and negative, represent the proportion of the text that falls into those categories.  The final metric, the compound score, is the sum of all of the lexicon ratings which have been standardised to range between -1 and 1. Moreover, VADER doesn’t just do simple matching between the words in the text and in its lexicon. It also considers certain things about the way the words are written as well as their context. One of the things that VADER recognises is capitalisation, which increases the intensity of both positive and negative words. Another factor that increases the intensity of sentence sentiment is exclamation marks, with up to 3 exclamation marks adding additional positive or negative intensity. VADER also takes into account what happens when modifying words are present in front of a sentiment term. Finally, VADER also handles changes in a sentence’s sentiment intensity when it contains ‘but’. Essentially, the rule is that the sentiments expressed both before and after the ‘but’ are taken into consideration, but the sentiment afterwards is weighted more heavily than that before.\n",
    "\n",
    "This library takes into account :\n",
    "\n",
    "- positive sentence example\n",
    "- punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "- booster words handled correctly (sentiment intensity adjusted)\n",
    "- emphasis for ALLCAPS handled\n",
    "- combination of signals - VADER appropriately adjusts intensity\n",
    "- booster words & punctuation make this close to ceiling for score\n",
    "- negation sentence example\n",
    "- positive sentence\n",
    "- negated negative sentence with contraction\n",
    "- qualified positive sentence is handled correctly (intensity adjusted)\n",
    "- mixed negation sentence\n",
    "- negative slang with capitalization emphasis\n",
    "- mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "- emoticons handled\n",
    "- emojis handled\n",
    "- Capitalized negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_analysis(dict_per_):\n",
    "    i = 0\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    dict_score = dict()\n",
    "    for key in list(dict_per_.keys()):\n",
    "        print(f'{i} over {len(dict_per_.keys())}')\n",
    "        neg, pos, neu, compound, tmp_dict = [], [], [], [], dict()\n",
    "        for sentence in dict_per_[key]:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            compound.append(vs['compound'])\n",
    "        tmp_dict['compound'] = np.mean(compound)\n",
    "        dict_score[key] = tmp_dict \n",
    "        i=i+1\n",
    "        \n",
    "    return dict_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vader_analysis(dict_per_compagny)\n",
    "pickle.dump(output, open('dictionary_per_compagny_score_vader.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dictionary_per_compagny_score_vader.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vader_analysis(dict_per_investor)\n",
    "pickle.dump(output, open('dictionary_per_investor_score_vader.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText: \n",
    "\n",
    "The second one is FastText. To be efficient on datasets with very large number of categories, it uses a hierarchical classifier instead of a flat structure, in which the different categories are organized in a tree (think binary tree instead of list). This reduces the time complexities of training and testing text classifiers from linear to logarithmic with respect to the number of classes. FastText also exploits the fact that classes are imbalanced (some classes appearing more often than other) by using the Huffman algorithm to build the tree used to represent categories. The depth in the tree of very frequent categories is therefore smaller than for infrequent ones, leading to further computational efficiency.FastText also represents a text by a low dimensional vector, which is obtained by summing vectors corresponding to the words appearing in the text. In fastText, a low dimensional vector is associated to each word of the vocabulary. This hidden representation is shared across all classifiers for different categories, allowing information about words learned for one category to be used by other categories. These kind of representations, called bag of words, ignore word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR_PATH = \"../../data_twitter/\"\n",
    "\n",
    "# Load a pretrained model for classification\n",
    "model = fastText.load_model(os.path.join(MODEL_DIR_PATH, \"amazon_review_full.bin\"))\n",
    "\n",
    "maketrans = str.maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment = lambda s: model.predict(clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastext_prediction(dict_per_):\n",
    "    dict_score = dict()\n",
    "    i=0\n",
    "    for key in list(dict_per_.keys()):\n",
    "        print(f'{i} over {len(dict_per_.keys())}')\n",
    "        label, confidence, tmp_dict = [], [], dict()\n",
    "        for sentence in dict_per_[key]:\n",
    "            res = predict_sentiment(sentence)\n",
    "            label.append(int(res[0][0][9]))\n",
    "            confidence.append(res[1][0])\n",
    "        tmp_dict['label'] = np.mean(label)\n",
    "        tmp_dict['confidence'] = np.mean(confidence)\n",
    "        dict_score[key] = tmp_dict \n",
    "        i=i+1\n",
    "\n",
    "    return dict_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fastext_prediction(dict_per_compagny)\n",
    "pickle.dump(output, open('dictionary_per_compagny_score_fasttext.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fastext_prediction(dict_per_investor)\n",
    "pickle.dump(output, open('dictionary_per_investor_score_fasttext.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation Part\n",
    "\n",
    "### Preprocessing : Import scores and plot the distribution  :\n",
    "\n",
    "In this part, we are going pre-process the data to have a good format to allows the usage of Gephi to make some grah plot. To do that we will first create dataframe per type of error and per category (investor/compagny). Then, a third scores will be created : the mean of the both normalized scores. We normalized it to give the same importance to each metrics. Then, merging of compagny and investor data will be done. The categorical features is kept by creating a category features with two different string : 'investor' and 'compagny'. To conclude this preprocessing part, node and edge csv files will be created to allows a easy usage of Gephi tool. A correlation dataframe will also be created to allow the creation of a scatter plot. This dataframe will contain Just Capital score and sentiment analysis scores for all compagnies/investors.\n",
    "\n",
    "In this first part, dataframe creation, normalization of fasttext score and merging both metrics will be done.\n",
    "\n",
    "### Compagnies scores\n",
    "\n",
    "#### Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dictionary_per_compagny_score_vader.p','rb'))\n",
    "dataframe_compagny = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_key, list_score = [],[]\n",
    "for key in df.keys():\n",
    "    list_key.append(key)\n",
    "    list_score.append(df[key]['compound'])\n",
    "dataframe_compagny['compagny'] = list_key\n",
    "dataframe_compagny['Vader_score'] = list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "data = [go.Histogram(x=dataframe_compagny.Vader_score)]\n",
    "py.iplot(data, filename='Vader Score histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dictionary_per_compagny_score_fastext.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "list_key, list_score, list_confidence = [],[],[]\n",
    "for key in df.keys():\n",
    "    list_key.append(key)\n",
    "    list_score.append(df[key]['label']) \n",
    "    list_confidence.append(df[key]['confidence'])\n",
    "dataframe_compagny['compagny'] = list_key\n",
    "dataframe_compagny['FastText_score'] = list_score\n",
    "dataframe_compagny['FastText_confidence'] = list_confidence\n",
    "# We normalize the score\n",
    "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(dataframe_compagny[['FastText_score','FastText_confidence']].fillna(0))\n",
    "dataframe_compagny[['FastText_score','FastText_confidence']] = scaler.transform(dataframe_compagny[['FastText_score','FastText_confidence']].fillna(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "score = go.Histogram(x=dataframe_compagny.FastText_score, opacity=0.8)\n",
    "confidence = go.Histogram(x=dataframe_compagny.FastText_confidence, opacity=0.5)\n",
    "\n",
    "data = [score, confidence]\n",
    "layout = go.Layout(barmode='overlay')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='FastTest Score and confidence histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Investors scores :\n",
    "\n",
    "#### Vader :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dictionary_per_investor_score_vader.p','rb'))\n",
    "dataframe_investor = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_key, list_score = [],[]\n",
    "for key in df.keys():\n",
    "    list_key.append(key)\n",
    "    list_score.append(df[key]['compound'])\n",
    "dataframe_investor['compagny'] = list_key\n",
    "dataframe_investor['Vader_score'] = list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "data = [go.Histogram(x=dataframe_investor.Vader_score)]\n",
    "py.iplot(data, filename='Vader Score histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('dictionary_per_investor_score_fastext.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_key, list_score, list_confidence = [],[],[]\n",
    "for key in df.keys():\n",
    "    list_key.append(key)\n",
    "    list_score.append(df[key]['label'])\n",
    "    list_confidence.append(df[key]['confidence'])\n",
    "dataframe_investor['compagny'] = list_key\n",
    "dataframe_investor['FastText_score'] = list_score\n",
    "dataframe_investor['FastText_confidence'] = list_confidence\n",
    "\n",
    "# We normalize the score\n",
    "scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = scaler.fit(dataframe_investor[['FastText_score','FastText_confidence']].fillna(0))\n",
    "dataframe_investor[['FastText_score','FastText_confidence']] = scaler.transform(dataframe_investor[['FastText_score','FastText_confidence']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = go.Histogram(x=dataframe_investor.FastText_score, opacity=0.75)\n",
    "confidence = go.Histogram(x=dataframe_investor.FastText_confidence, opacity=0.75)\n",
    "\n",
    "data = [score, confidence]\n",
    "layout = go.Layout(barmode='overlay')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='FastTest Score and confidence histogram for compagnies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate steps \n",
    "pickle_dump(dataframe_investor,'dataframe_investor_both_scores.p')\n",
    "pickle_dump(dataframe_compagny,'dataframe_compagny_both_scores.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution histogram for both scores\n",
    "\n",
    "#### Compagnies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_score = go.Histogram(x=dataframe_compagny.FastText_score, opacity=0.75)\n",
    "vd_score = go.Histogram(x=dataframe_compagny.Vader_score, opacity=0.75)\n",
    "confidence = go.Histogram(x=dataframe_compagny.FastText_confidence, opacity=0.75)\n",
    "\n",
    "data = [ft_score, vd_score, confidence]\n",
    "layout = go.Layout(barmode='overlay')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Both Score histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_score = go.Histogram(x=dataframe_investor.FastText_score, opacity=0.75)\n",
    "vd_score = go.Histogram(x=dataframe_investor.Vader_score, opacity=0.75)\n",
    "confidence = go.Histogram(x=dataframe_investor.FastText_confidence, opacity=0.75)\n",
    "\n",
    "data = [ft_score, vd_score, confidence]\n",
    "layout = go.Layout(barmode='overlay')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Both Score histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge investors and compagnies in the same dataframe\n",
    "Doing this process will help us doing the following graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this features for categories different categories in the futur plots.\n",
    "dataframe_investor['category'] = 'investor'\n",
    "dataframe_compagny['category'] = 'compagny'\n",
    "dataframe_final_scores = pd.concat([dataframe_compagny, dataframe_investor], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_final_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_score = go.Histogram(x=dataframe_final_scores.FastText_score, opacity=0.75)\n",
    "vd_score = go.Histogram(x=dataframe_final_scores.Vader_score, opacity=0.75)\n",
    "confidence = go.Histogram(x=dataframe_final_scores.FastText_confidence, opacity=0.75)\n",
    "\n",
    "data = [ft_score, vd_score, confidence]\n",
    "layout = go.Layout(barmode='overlay')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Both Score histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge both scores in a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_final_scores['Scores_Total'] = dataframe_final_scores[['Vader_score','FastText_score']].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_final_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_score = go.Histogram(x=dataframe_final_scores.FastText_score, opacity=0.75)\n",
    "vd_score = go.Histogram(x=dataframe_final_scores.Vader_score, opacity=0.75)\n",
    "confidence = go.Histogram(x=dataframe_final_scores.FastText_confidence, opacity=0.30)\n",
    "tot_score = go.Histogram(x=dataframe_final_scores.Scores_Total, opacity=0.75)\n",
    "\n",
    "data = [ft_score, vd_score, confidence, tot_score]\n",
    "layout = go.Layout(barmode='overlay')\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Both Score histogram for compagnies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create node dataframe for the merge data with the 3 different type of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, we prepare the dataset to be use in Gephi to do beautiful graph plot\n",
    "dataframe_final_scores.category = dataframe_final_scores.category.astype('category')\n",
    "dataframe_final_scores = dataframe_final_scores[['compagny', 'Vader_score', 'FastText_score', 'FastText_confidence','Score_Total', 'category']]\n",
    "dataframe_final_scores = dataframe_final_scores.rename(columns = {'compagny' : 'ID'}).reset_index().drop(['index'], axis =1)\n",
    "dataframe_final_scores = dataframe_final_scores.set_index('ID')\n",
    "dataframe_final_scores = dataframe_final_scores.groupby(dataframe_final_scores.index).sum()\n",
    "dataframe_final_scores.head() ####attention dans rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_final_scores.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(dataframe_final_scores,'node_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the corresponding edge matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean load data and select data of interest (which means for year 2017)\n",
    "df_edges = pd.read_csv('../../Desktop/13f_results_13-18Q3_merged.csv')\n",
    "print(len(df_edges))\n",
    "df_edges['date'] = pd.to_datetime(df_edges['date'])\n",
    "df_edges['Year'] = df_edges['date'].dt.year\n",
    "df_edges = df_edges[df_edges.Year == 2017]\n",
    "df_edges = df_edges.reset_index()\n",
    "print(len(df_edges))\n",
    "# Select meaning full features\n",
    "df_edges = df_edges[['investor','date','stock','name', 'industry','scores.overall']]\n",
    "# Create the edges matrix by cleaning it\n",
    "for i in range(len(df_edges)):\n",
    "    index_name = clean_text(df_edges['name'][i]).split().index(max(clean_text(df_edges['name'][i]).split(), key=len))\n",
    "    if index_name > 0:\n",
    "        df_edges['name'][i] = f'{clean_text(df_edges.name[i]).split()[index_name-1]} {clean_text(df_edges.name[i]).split()[index_name]}'\n",
    "    else :\n",
    "        df_edges['name'][i] = clean_text(df_edges.name[i]).split()[0]\n",
    "    \n",
    "    index_investor = clean_text(df_edges['investor'][i]).split().index(max(clean_text(df_edges['investor'][i]).split(), key=len))\n",
    "    elif index_investor > 1 :\n",
    "        df_edges['investor'][i] = f'{clean_text(df_edges.investor[i]).split()[index_investor-2]} {clean_text(df_edges.investor[i]).split()[index_investor-1]} {clean_text(df_edges.investor[i]).split()[index_investor]}'\n",
    "    elif index_investor > 0 :\n",
    "        df_edges['investor'][i] = f'{clean_text(df_edges.investor[i]).split()[index_investor-1]} {clean_text(df_edges.investor[i]).split()[index_investor]}' \n",
    "    else :\n",
    "        df_edges['investor'][i] = clean_text(df_edges.investor[i]).split()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = df_edges.rename(columns = {'investor':'Source', 'name':'Target'})\n",
    "df_edges = df_edges.reset_index()[['Source','Target']]\n",
    "df_edges = df_edges.set_index(['Source'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_edges.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_dump(df_edges,'edge_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this new matrix to take just the node present in this edge matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A COMPLETER !!!!!! \n",
    "all_name\n",
    "dataframe_final_scores = dataframe_final_scores[dataframe_final_scores.index.isin()]\n",
    "pickle_dump(dataframe_final_scores,'node_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create correlation dataframe to produce the following scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_capital = []\n",
    "correlation = df_edges\n",
    "correlation['scores_sentiment_compagny'] = 0.0\n",
    "for i in range(correlation.shape[0]) :\n",
    "    for j in range (dataframe_final_scores.shape[0]) :\n",
    "        if correlation['name'][i] == dataframe_final_scores.index[j]:\n",
    "            correlation['scores_sentiment_compagny'][i] = float(dataframe_final_scores.loc[dataframe_final_scores.index == dataframe_final_scores.index[j],'Interval'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation.to_pickle('correlation.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot to analyse correlation between Just Capital scores and sentiment analysis scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trace\n",
    "trace = go.Scatter(\n",
    "    x = correlation['scores.overall'],\n",
    "    y = correlation['scores_sentiment_compagny'],\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "fig = dict(data=data)\n",
    "py.iplot(fig, filename='Correlation between Sentiment Analysis scores and JustCapital Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "### Graph plot exploration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {df_node.shape[0]} compagnies/investors linked with {df_edges.shape[0]} edges in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the edges information in the good format\n",
    "link_graph = nx.from_pandas_edgelist(df_edges, 'Source', 'Target', edge_attr=None, create_using= nx.Graph())\n",
    "# Set attributes\n",
    "nx.set_node_attributes(link_graph, df_node['Vader_score'].to_dict(), 'Vader_score' )\n",
    "nx.set_node_attributes(link_graph, df_node['FastText_score'].to_dict(), 'FastText_score' )\n",
    "link_graph.node['caterpillar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We connect the network\n",
    "pos = nx.spring_layout(link_graph,k=0.2)\n",
    "ec = nx.draw_networkx_edges(link_graph, pos, alpha=0.4)\n",
    "nc = nx.draw_networkx_nodes(link_graph, pos, nodelist=link_graph.nodes(), node_color='g', cmap=plt.cm.jet, node_shape='.')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is not a very pretty. That's why we installed Gephy for some better-looking plots.\n",
    "Let's answer the classical question of a graph analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Network sparsity: %.4f\" %nx.density(link_graph))\n",
    "print(nx.is_connected(link_graph))\n",
    "comp = list(nx.connected_components(link_graph))\n",
    "print('The graph contains', len(comp), 'connected components')\n",
    "largest_comp = max(comp, key=len)\n",
    "percentage_lcc = len(largest_comp)/ link_graph.number_of_nodes() * 100\n",
    "print('The largest component has ', len(largest_comp), 'nodes', 'accounting for %.2f'% percentage_lcc, '% of the nodes')\n",
    "path = nx.shortest_path(link_graph, source=\"caterpillar\", target=\"hca\")\n",
    "print(\"Shortest path between caterpillar and hca:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visually inspecting the graph, we already saw that most of the nodes are in the largest component, which is often the case in a graph. That is why we call that a giant component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the largest component and analyse its diameter = longest shortest path \n",
    "lcc_link_graph = link_graph.subgraph(largest_comp)\n",
    "diameter = nx.diameter(lcc_link_graph)\n",
    "print(\"The diameter of the largest component is\", diameter, \"which is the longest shortest path between any two compagnies in the giant component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a subgraph to see the form :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_hca = link_graph.subgraph(['apple']+list(link_graph.neighbors('apple')))\n",
    "nx.draw_spring(subgraph_hca, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which compagny/investors are the most important ?\n",
    "There are many ways to detect important nodes in a graph, for example based on degree or betweeness centrality.\n",
    "\n",
    "Let's begin with the degree : This view is that the more compagnies you are connected with, the more important you are. We will look at the degree distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(link_graph.degree(link_graph.nodes()))\n",
    "sorted_degree = sorted(degrees.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "# And the top 5 most popular quakers are.. \n",
    "for compagny, degree in sorted_degree[:5]:\n",
    "    print(compagny, ' who is', link_graph.node[compagny]['Score'], 'is linked', degree, 'compganies/investors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_seq = [d[1] for d in sorted_degree]\n",
    "degreeCount = collections.Counter(degree_seq)\n",
    "degreeCount = pd.DataFrame.from_dict( degreeCount, orient='index').reset_index()\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(degreeCount['index'], degreeCount[0], 'o', c='blue', markeredgecolor='none', markersize= 4)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Degree')\n",
    "plt.title('Degree distribution for the investors/compagnies network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now betweeness : The other view is that the more shortest paths pass through a node, the more important it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness = nx.betweenness_centrality(link_graph) # Run betweenness centrality\n",
    "# Assign each to an attribute in your network\n",
    "nx.set_node_attributes(link_graph, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "for quaker, bw in sorted_betweenness[:5]:\n",
    "    print(quaker, ' who is', link_graph.node[quaker]['Score'], 'has betweeness: %.3f' %bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar pattern\n",
    "list_nodes =list(link_graph.nodes())\n",
    "list_nodes.reverse()   # for showing the nodes with high betweeness centrality \n",
    "pos = nx.spring_layout(link_graph)\n",
    "ec = nx.draw_networkx_edges(link_graph, pos, alpha=0.1)\n",
    "nc = nx.draw_networkx_nodes(link_graph, pos, nodelist=list_nodes, node_color=[link_graph.nodes[n][\"betweenness\"] for n in list_nodes], \n",
    "                            with_labels=False, alpha=0.8, node_shape = '.')\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try clustering : The Louvain method is a clustering algorithm and has become a standard algorithm in the data scientist toolbox.\n",
    "\n",
    "It proceeds the other way around: initially every node is considered as a community. The communities are traversed, and for each community it is tested whether by joining it to a neighboring community, we can obtain a better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "partition = community_louvain.best_partition(link_graph)\n",
    "# add it as an attribute to the nodes\n",
    "for n in link_graph.nodes:\n",
    "    link_graph.nodes[n][\"louvain\"] = partition[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it out\n",
    "pos = nx.spring_layout(link_graph,k=0.2)\n",
    "ec = nx.draw_networkx_edges(link_graph, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(link_graph, pos, nodelist=link_graph.nodes(), node_color=[link_graph.nodes[n][\"louvain\"] for n in link_graph.nodes], \n",
    "                            with_labels=False, node_size=100, cmap=plt.cm.jet)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster= partition['apple']\n",
    "# Take all the nodes that belong to William's cluster\n",
    "members_c = [q for q in link_graph.nodes if partition[q] == cluster]\n",
    "# get info about these quakers\n",
    "for compagny in members_c:\n",
    "    print(compagny, ' who is', link_graph.node[compagny].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this analysis, we will calculate the Homophily which like correlation, but translated into graphs.\n",
    "\n",
    "It means : How likely two compagnies that have the same attribute(just_capital_score or sentiment_analysis_score) are linked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical attributes\n",
    "print (f'Homophily Vader_score :', nx.attribute_assortativity_coefficient(link_graph, 'Vader_score'))\n",
    "# for categorical attributes\n",
    "print (f'Homophily FastText_score :', nx.attribute_assortativity_coefficient(link_graph, 'FastText_score')\n",
    "# for categorical attributes\n",
    "print (f'Homophily Score_Total :', nx.attribute_assortativity_coefficient(link_graph, 'Score_Total')\n",
    "# for categorical attributes\n",
    "print (f'Homophily Just_Capital :', nx.attribute_assortativity_coefficient(link_graph, 'Just_Capital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots done with Gephi :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A COMPLETER\n",
    "Graph degré\n",
    "Graph Vader\n",
    "Graph Fastext\n",
    "Graph merge\n",
    "Graph just capital\n",
    "tenté map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rajouter endroit ou merge file\n",
    "De plus faire comme d hab avec ceux qui ne sont pas relié et enlever ces noeud soit avec filtre soit en faisant un is in pour les noeud --> objectif baisser le nb de noeud.\n",
    "problem category et re run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
